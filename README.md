# GestureTalk AI ğŸ¤Ÿ  
**AI-Powered Sign Language Interpreter, Translator & Learning Platform**

---

## ğŸ“Œ Project Overview

**GestureTalk AI** is an AI-powered web application designed to bridge communication and education gaps for deaf and hard-of-hearing individuals. The platform performs **real-time sign language interpretation and translation** using computer vision and machine learning, while also providing a **sign-first learning ecosystem**.

Beyond interpretation, GestureTalk AI enables users to **learn sign language fundamentals**, track learning progress, access **mentor-led academic courses**, and receive guidance through an integrated AI chatbot â€” all through an accessible web interface.

---

## ğŸ¯ Key Features

- ğŸ¤– **Real-time Sign Language Interpretation**
  - Interprets hand gesture images using AI
  - Converts sign language gestures into accurate text
  - Works with a standard camera (no special hardware required)

- ğŸ“š **Sign Language Learning Module**
  - Learn **alphabets, words, and numbers**
  - Short visual sign language videos
  - Automatic **progress tracking** for each user

- ğŸ“ **Inclusive Academic Courses**
  - Maths, English, and Science (Class 5â€“10)
  - Taught by mentors **entirely through sign language**
  - Accessible learning beyond basic sign language

- ğŸ’¬ **AI Chatbot Assistance**
  - Guides users across the platform
  - Helps with navigation and sign language-related queries

- ğŸ” **Login & Personalization**
  - User authentication
  - Personalized learning progress and experience

- ğŸ¨ **Theme Customization**
  - Light/Dark mode support for better accessibility

---

## ğŸ§  System Architecture

GestureTalk AI follows a **clean, modular, and scalable clientâ€“server architecture**:

- **Frontend:**  
  Built using **HTML, CSS, and JavaScript**  
  Handles UI, webcam input, learning modules, chatbot interface, and progress tracking.

- **Backend:**  
  Developed in **Python using FastAPI**  
  Manages API requests, communicates with the AI model, and ensures fast response times.

- **AI / ML Model:**  
  Built using **Python, TensorFlow, and Keras**  
  Performs hand gesture recognition using computer vision techniques and maps gestures to text outputs.

This architecture ensures real-time performance, scalability, and maintainability.

---

## ğŸ—‚ Project Structure
Project_Root/
â”‚
â”œâ”€â”€ Backend/                     
â”‚   â”œâ”€â”€ __pycache__/             <-- Auto-generated by Python (ignore this)
â”‚   â”œâ”€â”€ main.py                  <-- The FastAPI server code
â”‚   â””â”€â”€ requirements.txt         <-- List of libraries (fastapi, mediapipe, etc.)
â”‚
â”œâ”€â”€ Frontend/                    <-- TEAMMATE'S WORKSPACE
â”‚   â”œâ”€â”€ About_us/                <-- Feature folders...
â”‚   â”œâ”€â”€ Chatbot/
â”‚   â”œâ”€â”€ Contact_us/
â”‚   â”œâ”€â”€ Landingpage/
â”‚   â”‚   â”œâ”€â”€ home.html            <-- Likely the main entry point
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ Learning/
â”‚   â”œâ”€â”€ Login/
â”‚   â”œâ”€â”€ Pricing/
â”‚   â”œâ”€â”€ upload.html              <-- The new test page we created
â”‚   â””â”€â”€ index.html               <-- The main landing page
â”‚
â”œâ”€â”€ Model/                       <-- SHARED BRAIN
â”‚   â”œâ”€â”€ asl_model.h5             <-- The trained AI Model
â”‚   â”œâ”€â”€ labels.pickle            <-- The dictionary (0='A', 1='B'...)
â”‚   â””â”€â”€ run_model.py             <-- Script to test model on webcam locally
â”‚
â”œâ”€â”€ SignLanguageIMG/             <-- DATASET (Optional, can keep for reference)
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ logo.png
â”œâ”€â”€ README.md
â””â”€â”€ structure.txt



---

## âš™ï¸ Tech Stack

### ğŸŒ Frontend
- HTML  
- CSS  
- JavaScript  

### âš™ï¸ Backend
- Python  
- FastAPI  

### ğŸ¤– AI / ML
- TensorFlow  
- Keras  
- Computer Vision (hand gesture recognition)

---
## â­ Unique Selling Points (USP)

- **Sign-first approach:** GestureTalk AI is designed around sign language from the ground up, not adapted later.
- **Beyond translation:** In addition to real-time sign language interpretation, the platform focuses on learning and education.
- **Integrated learning ecosystem:** Users can learn sign language basics with structured modules and progress tracking.
- **Inclusive academic education:** Mentor-led courses in Maths, English, and Science (Classes 5â€“10) taught entirely through sign language.
- **AI-powered architecture:** Uses a scalable web frontend, FastAPI backend, and TensorFlowâ€“Keras model for real-time performance.
- **No special hardware required:** Works with a standard camera, making it accessible and easy to use.
- **Built-in AI chatbot:** Provides instant guidance, support, and platform navigation assistance.

---

## â–¶ï¸ How to Run the Project

### 1ï¸âƒ£ Backend Setup
bash
-cd Backend
-pip install -r requirements.txt
-uvicorn main:app --reload

### 2ï¸âƒ£ Frontend Setup
-Navigate to the Frontend folder
-Open index.html (or Landingpage/home.html) in a web browser
-Ensure the backend server is running for API communication

### 3ï¸âƒ£ Model Testing (Optional)
To test the trained model locally using a webcam:
-cd Model
-python run_model.py

---

## ğŸ Conclusion
GestureTalk AI is not just a sign language interpreterâ€”it is a complete, AI-powered ecosystem for communication, learning, and inclusive education. By combining real-time gesture interpretation, structured sign language learning, mentor-led academic courses, and intelligent user support, GestureTalk AI aims to bridge communication and education gaps and move closer to a future where accessibility is the standard, not an exception.

